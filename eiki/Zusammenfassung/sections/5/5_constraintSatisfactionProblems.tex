\documentclass[../../eiki_summary.tex]{subfiles}

\externaldocument[ext:]{../../eiki_summary}
% Set Graphics Path, so pictures load correctly
\graphicspath{{../../pics/}}

\begin{document}

\section{Constraint Satisfaction Problems (CSPs)}

Standard search algorithms (like A* or BFS) treat states as atomic black boxes—they search for a \textit{sequence} of actions (a path) to a goal. In \defc{Constraint Satisfaction Problems (CSPs)}, the path is irrelevant. We treat states as \textbf{factored representations} (sets of variables and values) and simply search for a \defc{goal state} that satisfies all requirements.

\begin{defbox}[Definition: CSP]
  A CSP is defined by a triplet $(X, D, C)$:
  \begin{itemize}
    \item \textbf{Variables ($X$):} A finite set of variables $\{X_1, \dots, X_n\}$.
    \item \textbf{Domains ($D$):} A set of domains $\{D_1, \dots, D_n\}$, where each variable $X_i$ must take a value from the discrete set $D_i$.
    \item \textbf{Constraints ($C$):} A set of constraints specifying allowable combinations of values for subsets of variables.
  \end{itemize}
\end{defbox}

\subsection{Types of Assignments}
\begin{itemize}
  \item \textbf{Partial Assignment:} A state where values are assigned to only a subset of variables.
  \item \textbf{Consistent (Legal) Assignment:} An assignment that does not violate any constraints among the assigned variables.
  \item \textbf{Complete Assignment:} Every variable in $X$ is assigned a value.
  \item \defc{Solution:} A \textbf{complete} and \textbf{consistent} assignment.
\end{itemize}

\begin{center}
  \vcentered{\includegraphics[width=0.8\textwidth]{eiki_1_AI101-05_page_22_2.png}}
\end{center}

\subsection{Constraint Graphs}
To visualize the structure of a CSP, we use a \defc{Constraint Graph}. This abstraction is crucial because the topology of the graph (e.g., whether it contains loops or is a tree) dictates the complexity of solving it.
\begin{itemize}
  \item \textbf{Nodes:} Represent the variables $X_i$.
  \item \textbf{Edges:} Connect any two variables that participate in the same constraint.
\end{itemize}



\subsection{Types of Constraints}
\begin{enumerate}
  \item \textbf{Unary Constraint:} Restricts the value of a single variable (e.g., $SA \neq \text{green}$). These can often be processed simply by filtering the domain $D_i$ before search begins.
  \item \textbf{Binary Constraint:} Relates two variables (e.g., $SA \neq WA$). These form the edges of the Constraint Graph.
  \item \textbf{Higher-order Constraint:} Involves 3 or more variables (e.g., Cryptarithmetic puzzles like $TWO + TWO = FOUR$, where columns depend on carry bits).
  \item \textbf{Soft Constraints (Preferences):} Constraints that are not mandatory but preferred (e.g., "Red is better than Green"). This shifts the problem from standard CSP to \defc{Constrained Optimization}, often solved via cost functions.
\end{enumerate}

\subsection{Solving CSPs: Search Strategies}

We can model a CSP as a standard search problem:
\begin{itemize}
  \item \textbf{Initial State:} Empty assignment $\{\}$.
  \item \textbf{Successor Function:} Assign a value to an unassigned variable.
  \item \textbf{Goal Test:} Current assignment is complete and consistent.
\end{itemize}

\subsubsection{Commutativity and Search Space}
A naïve Breadth-First or Depth-First search would branch on every variable and every value in every order, creating a factorial search space ($n! \cdot d^n$).

However, CSPs are \defc{commutative}: The order in which we assign variables does not change the final state (assigning $WA=\text{red}$ then $NT=\text{green}$ leads to the same state as $NT=\text{green}$ then $WA=\text{red}$).
\begin{itemize}
  \item \textbf{Implication:} We fix the order of variables or choose only \textit{one} variable to branch on at each depth.
  \item \textbf{Benefit:} The search tree depth is fixed at $n$ (number of variables). The search space reduces to $d^n$.
\end{itemize}

\subsubsection{Backtracking Search}
Backtracking Search is the fundamental algorithm for solving CSPs. It is essentially a Depth-First Search (DFS) that operates on partial assignments. Unlike standard search where actions lead to new states, here an action is the assignment of a value to a variable.

\begin{defbox}[Backtracking Algorithm Logic]
  The algorithm relies on the \textbf{commutativity} of assignments (the order in which we assign variables doesn't matter for the final solution). This allows us to consider only a single variable at each node of the search tree, drastically reducing the branching factor from $n! \cdot d^n$ to $d^n$.
\end{defbox}

\textbf{Algorithm Steps:}
\begin{enumerate}
  \item \textbf{Base Case:} If the assignment is complete (all variables have values), return the assignment as the solution.
  \item \textbf{Variable Selection:} Select an unassigned variable using a specific strategy (see Heuristics).
  \item \textbf{Value Iteration:} Iterate through the values in the domain of the selected variable.
  \item \textbf{Consistency Check:} For each value, check if assigning it violates any constraints with currently assigned variables.
  \item \textbf{Recursive Step:}
        \begin{itemize}
          \item If consistent: Add $\{var = value\}$ to the assignment.
          \item Call \textsc{Recursive-Backtracking} again.
          \item If the recursive call returns a success, return that result.
          \item \textbf{Backtrack:} If the recursive call fails, remove $\{var = value\}$ from the assignment and try the next value in the domain.
        \end{itemize}
  \item \textbf{Failure:} If all values have been tried and none work, return failure.
\end{enumerate}

\subsection{Heuristics: Improving Backtracking}

Pure backtracking is slow. We use heuristics to decide \textit{which} variable to pick and \textit{which} value to try, effectively pruning the tree.

\begin{enumerate}
  \item Which variable to assign next? (\textit{Fail-First})
  \item In what order to try values? (\textit{Fail-Last})
  \item Can we detect inevitable failure early? (\textit{Inference})
\end{enumerate}

\subsubsection{Variable Selection (Which variable next?)}
\begin{enumerate}
  \item \defc{Minimum Remaining Values (MRV):}
        \begin{itemize}
          \item \textbf{Strategy:} Choose the variable with the \textit{fewest} legal values remaining.
          \item \textbf{Intuition ("Fail-First"):} If a variable has only 1 legal value left, we must assign it now. If we wait, it might become 0, causing a failure deeper in the tree. We want to force failures as high up in the tree as possible to prune large branches.
        \end{itemize}

  \item \defc{Degree Heuristic:}
        \begin{itemize}
          \item \textbf{Strategy:} Choose the variable involved in the largest number of constraints with \textit{other unassigned} variables.
          \item \textbf{Usage:} Often used as a tie-breaker for MRV.
          \item \textbf{Intuition:} Assigning the most connected variable exerts the maximum "pressure" on the rest of the graph, reducing the branching factor for future steps.
        \end{itemize}
\end{enumerate}

\subsubsection{Value Selection (Which value first?)}
\begin{enumerate}
  \item \defc{Least Constraining Value (LCV):}
        \begin{itemize}
          \item \textbf{Strategy:} Given a variable, try the value that rules out the \textit{fewest} values in the domains of neighboring variables.
          \item \textbf{Intuition ("Fail-Last"):} We want to find \textit{a} solution, not all solutions. Therefore, we should pick the path most likely to succeed by leaving the maximum flexibility for the remaining variables.
        \end{itemize}
\end{enumerate}

\subsection{Constraint Propagation (Inference)}

Search implies "trying" values and undoing them if they fail. \defc{Propagation} implies logically deducing which values are impossible and removing them \textit{before} we try them.

\subsubsection{Levels of Consistency}
\begin{itemize}
  \item \textbf{Node Consistency:} Every variable satisfies its unary constraints.
  \item \textbf{Arc Consistency:} A variable $X$ is arc-consistent with respect to $Y$ if, for every value $x \in D_X$, there is some value $y \in D_Y$ satisfying the binary constraint $(X, Y)$.
  \item \textbf{Path Consistency:} Ensures consistency for triples of variables.
\end{itemize}

Constraint propagation is the process of using constraints to reduce the legal domain of a variable, which in turn reduces the domains of its neighbors, and so on. This happens \textit{before} or \textit{during} search to reduce the search space.

\subsubsection{Algorithms for Propagation}

\paragraph{Forward Checking}
Forward checking is a simple form of propagation performed during backtracking search.

\textbf{Algorithm Steps:}
\begin{enumerate}
  \item When variable $X$ is assigned value $v$:
  \item Look at all unassigned variables $Y$ that are connected to $X$ by a constraint.
  \item Remove any value from $D_Y$ that conflicts with $X=v$.
  \item \textbf{Early Termination:} If any domain $D_Y$ becomes empty, stop this branch immediately (backtrack).
\end{enumerate}
\textit{Limitation:} It only checks direct neighbors. It does not detect if the reduction in $Y$'s domain makes $Y$ incompatible with a third variable $Z$.
\paragraph{Arc Consistency (AC-3 Algorithm)}
AC-3 propagates constraints globally. It ensures that every arc in the graph is consistent.

\begin{defbox}[AC-3 Algorithm]
  AC-3 is a more powerful general-purpose algorithm that propagates constraints globally until the network is \defc{Arc Consistent}.
  A variable $X$ is arc-consistent with respect to $Y$ if for every value $x \in D_X$, there is some allowed value $y \in D_Y$.

  \begin{enumerate}
    \item \textbf{Queue Initialization:} Create a queue containing all arcs (binary constraints) in the CSP: $\{(X_i, X_j), (X_j, X_i), \dots\}$.
    \item \textbf{While Queue is not empty:}
          \begin{itemize}
            \item Pop an arc $(X_i, X_j)$ from the queue.
            \item Call \textsc{Remove-Inconsistent-Values}($X_i, X_j$).
            \item \textbf{If values were removed} from $D_i$:
                  \begin{itemize}
                    \item The domain of $X_i$ has become smaller. This might ruin consistency for its neighbors.
                    \item Add all arcs $(X_k, X_i)$ (where $X_k$ is a neighbor of $X_i$) back into the queue.
                  \end{itemize}
          \end{itemize}
  \end{enumerate}
\end{defbox}

\textbf{Function \textsc{Remove-Inconsistent-Values}($X_i, X_j$):}
\begin{itemize}
  \item Iterate through every value $x$ in $D_i$.
  \item Check if there exists a value $y$ in $D_j$ that satisfies the constraint between $X_i$ and $X_j$.
  \item If no such $y$ exists, delete $x$ from $D_i$. Return \textit{true} (indicating change occurred).
\end{itemize}

\subsection{Local Search for CSPs}

Local Search algorithms (like Hill Climbing) operate on \defc{complete states}, meaning all variables are assigned a value at all times, even if the assignment is inconsistent (constraints are violated). The goal is to iteratively repair the assignment.

\textbf{Algorithm Steps:}
\begin{enumerate}
  \item \textbf{Initialization:} Start with a complete assignment for all variables (usually generated randomly).
  \item \textbf{Loop} (until a solution is found or max steps reached):
        \begin{itemize}
          \item Check if the current assignment satisfies all constraints. If yes, return it.
          \item \textbf{Variable Selection:} Randomly select a variable that is currently involved in a conflict (violating a constraint).
          \item \textbf{Value Selection (Minimization):} Choose a new value for this variable that minimizes the number of conflicts with other variables.
          \item Update the variable to this new value.
        \end{itemize}
\end{enumerate}

\textit{Note:} Like other local search methods, this can get stuck in local optima (plateaus), specifically when the ratio of constraints to variables is critical.

\subsection{Problem Structure and Decomposition}

\subsubsection{Independent Subproblems}
If the constraint graph consists of connected components that are disjoint, we can solve each component independently.
\begin{itemize}
  \item \textbf{Impact:} Reduces complexity from exponential in total variables $O(d^n)$ to exponential in the size of the largest component $O(d^c)$.
\end{itemize}

\subsubsection{Tree-Structured CSPs}
If the constraint graph forms a tree (no loops), we can solve the CSP in linear time $O(n \cdot d^2)$ instead of exponential time.

\textbf{Algorithm Steps:}
\begin{enumerate}
  \item \textbf{Topological Sort:} Linearlize the variables (order them $X_1, \dots, X_n$) such that every variable appears after its parent.
  \item \textbf{Backward Pass (Consistency):}
        \begin{itemize}
          \item Iterate from $j = n$ down to $2$.
          \item Apply arc consistency to the arc $(Parent(X_j), X_j)$.
          \item This ensures that for every value in the parent's domain, there is a valid value in the child's domain.
        \end{itemize}
  \item \textbf{Forward Pass (Assignment):}
        \begin{itemize}
          \item Iterate from $i = 1$ to $n$.
          \item Assign any value to $X_i$ that is consistent with the assignment of its parent.
          \item Because of the backward pass, a valid assignment is guaranteed to exist. NO backtracking is required.
        \end{itemize}
\end{enumerate}

\subsubsection{Nearly Tree-Structured Problems (Cutset Conditioning)}
Most real-world problems are not trees, but often "close" to trees. We can exploit this via \defc{Cutset Conditioning}. This technique is used for constraint graphs that are \textit{nearly} trees. It turns a cyclic graph into a tree by removing specific nodes.

\begin{itemize}
  \item \textbf{Cycle Cutset:} A subset of variables $S$ such that removing them renders the remaining graph a tree.
\end{itemize}

\textbf{Algorithm:}
\begin{enumerate}
  \item Identify the Cutset $S$.
  \item Iterate through all possible consistent assignments for variables in $S$.
  \item For each assignment of $S$, the values are fixed. This simplifies the constraints on the remaining variables.
  \item Solve the remaining variables (which now form a tree) using the efficient Tree CSP algorithm.
  \item \textbf{Complexity:} $O(d^{|S|} \cdot (n-|S|)d^2)$. Efficient if the cutset $|S|$ is small.
\end{enumerate}

\begin{center}
  \vcentered{\includegraphics[width=0.8\textwidth]{eiki_1_AI101-05_page_53_1.png}}
\end{center}

\end{document}