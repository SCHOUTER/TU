\documentclass[../../eiki_summary.tex]{subfiles}

\externaldocument[ext:]{../../eiki_summary}
% Set Graphics Path, so pictures load correctly
\graphicspath{{../../pics/}}

\begin{document}

\section{Local Search and Adversarial Search}

In previous lectures, the focus was on finding a sequence of actions (a path) to reach a goal (e.g., A* search). However, in many AI problems, the path to the solution is irrelevant; only the final state matters.

\begin{defbox}[Optimization Problem]
  An \defc{Optimization Problem} is a problem where all states are technically solutions, but they have different qualities. The goal is not to reach a state, but to find the \defc{best state} (Global Optimum) according to an \defc{Objective Function}.
\end{defbox}

\paragraph{The State Landscape}
Imagine the state space as a physical landscape where "elevation" corresponds to the value of the objective function.


\begin{itemize}
  \item \defc{Global Maximum}: The highest peak in the entire state space. The optimal solution.
  \item \defc{Local Maximum}: A peak that is higher than all its immediate neighbors but lower than the global maximum.
  \item \defc{Shoulder}: A flat region (plateau) that can be an intermediate step rising to a peak.
  \item \defc{Flat Local Maximum}: A plateau that is a local maximum.
\end{itemize}

\subsection{Local Search Algorithms}

Local search algorithms operate by maintaining a single \defc{Current State} and iteratively moving to neighboring states.

\begin{itemize}
  \item \textbf{Advantages:} Uses very little memory (constant space) and can find reasonable solutions in infinite/massive search spaces.
  \item \textbf{Disadvantages:} No guarantee of completeness (finding a solution) or optimality (finding the best solution).
\end{itemize}

\subsubsection{Hill Climbing (Greedy Local Search)}
The Hill Climbing algorithm is a loop that continually moves in the direction of increasing value (uphill).

\begin{enumerate}
  \item Generate all neighbors of the current state.
  \item Move to the neighbor with the highest value.
  \item Terminate when no neighbor has a higher value than the current state.
\end{enumerate}

\begin{defbox}[Metaphor]
  Hill climbing is like "climbing Mount Everest in thick fog with amnesia." You only know the slope under your feet, not where the true peak is.
\end{defbox}

\paragraph{Problems with Hill Climbing}
\begin{itemize}
  \item \defc{Local Optima}: The algorithm halts at a local peak, missing the higher global peak elsewhere.
  \item \defc{Ridges}: A sequence of local maxima that is difficult for greedy algorithms to navigate. If the ridge rises diagonally but movement is restricted to cardinal directions (North/East), the algorithm may think it's at a peak because every single step leads downhill, even though the ridge continues up.
  \item \defc{Plateaus}: Flat areas where the algorithm cannot determine which direction is better, leading to random wandering.
\end{itemize}

\paragraph{Variants to Solve Local Optima}
\begin{itemize}
  \item \defc{Randomized Restart}: Run the algorithm multiple times from different random starting positions.
  \item \defc{Stochastic Hill Climbing}: Choose a successor randomly, weighted by the steepness of the uphill move (better moves have higher probability).
\end{itemize}

\subsubsection{Beam Search}
Instead of keeping just one state in memory, \defc{Beam Search} keeps track of $k$ states.
\begin{itemize}
  \item At each step, generate all successors of all $k$ states.
  \item Select the best $k$ successors from the complete list to be the new current states.
  \item This is different from $k$ random restarts because the "beams" (threads) can communicate/converge on the most promising area.
\end{itemize}

\subsubsection{Simulated Annealing}
This algorithm combines Hill Climbing with a Random Walk to escape local optima. It is inspired by metallurgy (heating metal and cooling it slowly to toughen it).

\begin{defbox}[Mechanism]
  Instead of always picking the best move, the algorithm sometimes accepts "bad" moves (downhill) to explore the space. The probability of accepting a bad move depends on the \defc{Temperature (T)}.
\end{defbox}

\textbf{The Process:}
\begin{itemize}
  \item \textbf{High T (Early phase):} The algorithm accepts bad moves frequently (exploring widely, behaving like a random walk).
  \item \textbf{Low T (Late phase):} The algorithm rarely accepts bad moves (exploiting the local area, behaving like hill climbing).
\end{itemize}

\textbf{The Probability Formula:}
If the move improves the situation ($\Delta E > 0$), accept it always. If the move is worse ($\Delta E < 0$), accept it with probability:
\[ P = e^{\frac{\Delta E}{T}} \]
As $T \to 0$, the probability of accepting a negative $\Delta E$ approaches 0.

\subsection{Gradient Descent}

When the state space is \defc{continuous} (rather than discrete steps), we use Gradient Descent. This is the foundation of training Neural Networks.

\begin{defbox}[Gradient]
  The gradient $\nabla J(\theta)$ is a vector of partial derivatives pointing in the direction of the steepest ascent.
\end{defbox}

To minimize a cost function $J(\theta)$:
\[ \theta_{new} = \theta_{old} - \lambda \nabla J(\theta) \]
Where $\lambda$ is the \defc{Learning Rate}.

\subsubsection{The Learning Rate ($\lambda$)}
This hyperparameter controls the step size.
\begin{itemize}
  \item \textbf{Too Small:} Convergence is extremely slow; might get stuck in local minima.
  \item \textbf{Too Large:} The algorithm may overshoot the minimum, oscillate, or diverge (move away from the solution).
\end{itemize}

\subsection{Brief Note: SAT and Complexity}
The Boolean Satisfiability Problem (SAT) is finding an assignment of variables that makes a logical formula true.
\begin{itemize}
  \item \defc{Cook's Theorem}: SAT is NP-Complete.
  \item \defc{NP-Complete}: Problems that are effectively "hardest" in NP. If you can solve one NP-Complete problem in polynomial time, you can solve all of them ($P=NP$).
  \item \textbf{GSAT:} A local search algorithm for SAT that flips variable assignments to minimize unsatisfied clauses.
\end{itemize}

\subsection{Adversarial Search (Games)}

This domain deals with multi-agent environments where agents have conflicting goals (Zero-Sum Games).

\begin{defbox}[Zero-Sum Game]
  A situation where one player's gain is exactly the other player's loss. The total utility remains constant (usually 0).
\end{defbox}

\subsubsection{Game Trees}
Unlike standard search trees, nodes represent game states, and levels alternate between players:
\begin{itemize}
  \item \defc{MAX}: The agent trying to maximize the utility value (Us).
  \item \defc{MIN}: The opponent trying to minimize the utility value (Enemy).
\end{itemize}

\subsubsection{The Minimax Algorithm}
The Minimax algorithm determines the optimal move by performing a Depth-First Search (DFS) to the end of the game tree and then reasoning \textbf{backwards} (bottom-up).


[Image of minimax game tree example]


\textbf{The Process:}
\begin{enumerate}
  \item \textbf{Dive (DFS):} Traverse the tree all the way down to the leaf nodes (terminal states).
  \item \textbf{Evaluate:} Assign utility values to the leaf nodes (e.g., Win=1, Loss=-1, or numerical scores).
  \item \textbf{Propagate (Backtracking):}
        \begin{itemize}
          \item If the parent is a \textbf{MAX} node: It looks at its children and picks the \textbf{highest} value.
          \item If the parent is a \textbf{MIN} node: It looks at its children and picks the \textbf{lowest} value.
        \end{itemize}
  \item \textbf{Root Decision:} When the values reach the root, MAX chooses the move leading to the highest value.
\end{enumerate}
\textit{Note:} If multiple moves have the same value, standard implementations often pick the one found first (left-most).

\textbf{Complexity:}
\begin{itemize}
  \item Time: $O(b^m)$ (Exponential). $b$ is branching factor, $m$ is max depth.
  \item Space: $O(bm)$ (Linear), due to Depth First Search nature.
\end{itemize}

\subsection{Alpha-Beta Pruning}

Because Game Trees grow exponentially ($10^{40}$ nodes for Chess), we cannot search the whole tree. \defc{Pruning} allows us to ignore branches that will certainly not be chosen, without affecting the final result.

\subsubsection{Concept: Alpha and Beta}
We maintain two values during the search that represent the "best guarantees" found so far on the path from the root.

\begin{itemize}
  \item \defc{$\alpha$ (Alpha)}: The best (highest) value \textbf{MAX} can guarantee so far.
        \begin{itemize}
          \item Initialized to $-\infty$.
          \item Updated only at \textbf{MAX} nodes (when a child returns a value higher than current $\alpha$).
        \end{itemize}
  \item \defc{$\beta$ (Beta)}: The best (lowest) value \textbf{MIN} can guarantee so far.
        \begin{itemize}
          \item Initialized to $+\infty$.
          \item Updated only at \textbf{MIN} nodes (when a child returns a value lower than current $\beta$).
        \end{itemize}
\end{itemize}

\subsubsection{The Pruning Condition}
A branch is pruned (cut off) immediately when:
\[ \alpha \ge \beta \]

\textbf{Why does this work?}
\begin{itemize}
  \item \textbf{Beta Cutoff (at MIN node):} If MIN finds a move that results in a value $v$ where $v \le \alpha$, MIN will choose that (or something even lower). However, the MAX player at the parent node already has a guaranteed option worth $\alpha$ from a previous branch. MAX will therefore \textbf{never} choose the path to this MIN node. The rest of MIN's children are irrelevant.
  \item \textbf{Alpha Cutoff (at MAX node):} Symmetrically, if MAX finds a move worth $v \ge \beta$, MIN (at the parent node) will never allow the game to reach this MAX node, because MIN already has a better option (worth $\beta$) elsewhere.
\end{itemize}

\textbf{Efficiency:} With optimal move ordering (checking best moves first), Alpha-Beta pruning effectively doubles the searchable depth, reducing complexity to $O(b^{m/2})$.

\subsection{Advanced Game Search}

\subsubsection{Evaluation Functions}
Since we cannot reach terminal nodes in complex games (Chess/Go), we cut off the search at a specific depth and use a \defc{Heuristic Evaluation Function}. This estimates the probability of winning from that state (e.g., Material value in chess: Pawn=1, Queen=9).

\subsubsection{Monte Carlo Tree Search (MCTS)}
Used in games like Go where branching factors are too high for Minimax ($b \approx 250$).
\begin{itemize}
  \item Instead of exhaustive search, it plays many random "simulation" games from the current state to the end.
  \item It builds statistics: "In 80\% of random games starting from move A, I won."
  \item \textbf{AlphaGo:} Combined MCTS with Deep Neural Networks (to predict move probabilities and value positions) to defeat the world champion.
\end{itemize}

\end{document}