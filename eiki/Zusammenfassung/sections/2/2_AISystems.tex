\documentclass[../../eiki_summary.tex]{subfiles}

\externaldocument[ext:]{../../eiki_summary}
% Set Graphics Path, so pictures load correctly
\graphicspath{{../../pics/}}

\begin{document}

\section{AI101-02: AI Systems}

Dieses Kapitel behandelt die grundlegenden Bausteine von KI-Systemen: Agenten und ihre Umgebungen. Es definiert Rationalität und klassifiziert verschiedene Arten von Umgebungen und Agentenarchitekturen.

\subsection{KI-Systeme und Agenten}

Ein KI-System wird definiert durch die Interaktion zwischen einem Agenten und seiner Umgebung.

\begin{defbox}[Definition: Agent]
   Ein \defc{Agent} ist eine Entität, die ihre Umgebung durch \defc{Sensoren} wahrnimmt und auf diese Umgebung durch \defc{Aktuatoren} (Effektoren) einwirkt.
\end{defbox}

\begin{center}
   \vcentered{\includegraphics[width=0.7\textwidth]{eiki_1_AI101-02_page_15_1.png}}
\end{center}

\begin{itemize}
   \item \textbf{Wahrnehmung (Percept):} Der sensorische Input des Agenten zu einem bestimmten Zeitpunkt.
   \item \textbf{Wahrnehmungssequenz (Percept Sequence):} Die vollständige Historie aller Wahrnehmungen, die der Agent bisher erhalten hat.
   \item \textbf{Agentenfunktion:} Eine mathematische Abbildung von Wahrnehmungssequenzen auf Aktionen: $f: P^* \rightarrow A$.
   \item \textbf{Agentenprogramm:} Die konkrete Implementierung der Agentenfunktion, die auf einer physischen Architektur läuft.
\end{itemize}

\subsection{Rationalität}

Ein rationaler Agent ist nicht zwangsläufig ein perfekter oder allwissender Agent. Rationalität bezieht sich auf die Qualität der Entscheidungsfindung basierend auf den verfügbaren Informationen.

\begin{defbox}[Rationaler Agent]
   Ein \defc{rationaler Agent} wählt für jede mögliche Wahrnehmungssequenz diejenige Aktion aus, die erwartungsgemäß sein Leistungsmaß (\textit{Performance Measure}) maximiert, unter Berücksichtigung der Wahrnehmungssequenz und des eingebauten Wissens.
\end{defbox}

Rationalität hängt von vier Faktoren ab:
\begin{enumerate}
   \item Dem \textbf{Leistungsmaß} (Performance Measure), das den Erfolg definiert.
   \item Dem \textbf{Wissen}, das der Agent bereits besitzt (Prior Knowledge).
   \item Den \textbf{Aktionen}, die der Agent ausführen kann.
   \item Der bisherigen \textbf{Wahrnehmungssequenz}.
\end{enumerate}

\textbf{Wichtige Unterscheidung:}
\begin{itemize}
   \item \textbf{Allwissenheit (Omniscience):} Kennt das tatsächliche Ergebnis jeder Aktion (in der Realität unmöglich).
   \item \textbf{Rationalität:} Maximiert das \textit{erwartete} Ergebnis basierend auf dem aktuellen Wissen.
\end{itemize}

\subsection{Task Environments (PEAS)}

Um einen Agenten zu entwerfen, muss die Aufgabenstellung spezifiziert werden. Dies geschieht durch das PEAS-Modell.

\begin{defbox}[PEAS]
   \begin{itemize}
      \item \textbf{P}erformance Measure (Leistungsmaß): Woran wird Erfolg gemessen?
      \item \textbf{E}nvironment (Umgebung): Wo operiert der Agent?
      \item \textbf{A}ctuators (Aktuatoren): Womit handelt der Agent?
      \item \textbf{S}ensors (Sensoren): Womit nimmt der Agent wahr?
   \end{itemize}
\end{defbox}

\textbf{Beispiel: Autonomes Taxi}
\begin{itemize}
   \item \textbf{P:} Sicherheit, Schnelligkeit, Legalität, Komfort, Profit.
   \item \textbf{E:} Straßen, anderer Verkehr, Fußgänger, Wetter.
   \item \textbf{A:} Lenkung, Gaspedal, Bremse, Hupe, Blinker.
   \item \textbf{S:} Kameras, Radar, Tacho, GPS, Motorensensoren.
\end{itemize}

\subsection{Eigenschaften von Umgebungen}

Die Art der Umgebung bestimmt maßgeblich das Design des Agenten. Umgebungen werden anhand folgender Dimensionen klassifiziert:

\begin{enumerate}
   \item \textbf{Fully Observable (Vollständig beobachtbar) vs. Partially Observable:}
         \begin{itemize}
            \item \textit{Fully:} Die Sensoren geben zu jedem Zeitpunkt den kompletten Zustand der Umgebung wieder (kein interner Speicher nötig).
            \item \textit{Partially:} Teile des Zustands sind verdeckt oder Sensoren sind ungenau (interner Zustand/Gedächtnis nötig).
         \end{itemize}

   \item \textbf{Single Agent vs. Multi-Agent:}
         \begin{itemize}
            \item \textit{Single:} Der Agent agiert allein (z.B. Kreuzworträtsel).
            \item \textit{Multi:} Es gibt andere Agenten, die kooperativ oder kompetitiv sein können (z.B. Schach, Straßenverkehr).
         \end{itemize}

   \item \textbf{Deterministic vs. Stochastic:}
         \begin{itemize}
            \item \textit{Deterministic:} Der nächste Zustand wird vollständig durch den aktuellen Zustand und die Aktion des Agenten bestimmt.
            \item \textit{Stochastic:} Es gibt Unsicherheiten/Zufall (z.B. Wetter, Würfelglück). Wenn Wahrscheinlichkeiten unbekannt sind, nennt man es \textit{non-deterministic}.
         \end{itemize}

   \item \textbf{Episodic vs. Sequential:}
         \begin{itemize}
            \item \textit{Episodic:} Die Erfahrung ist in atomare Episoden unterteilt. Eine Aktion in Episode $A$ hat keine Auswirkung auf Episode $B$ (z.B. Bildklassifizierung).
            \item \textit{Sequential:} Die aktuelle Entscheidung beeinflusst alle zukünftigen Entscheidungen (z.B. Schach, Fahren).
         \end{itemize}

   \item \textbf{Static vs. Dynamic:}
         \begin{itemize}
            \item \textit{Static:} Die Umgebung ändert sich nicht, während der Agent ``nachdenkt'' (z.B. Kreuzworträtsel).
            \item \textit{Dynamic:} Die Umgebung ändert sich kontinuierlich (z.B. Taxi fahren).
            \item \textit{Semi-dynamic:} Der Zustand ändert sich nicht, aber die Bewertung (Performance) ändert sich mit der Zeit (z.B. Schach mit Schachuhr).
         \end{itemize}

   \item \textbf{Discrete vs. Continuous:}
         \begin{itemize}
            \item Bezieht sich auf die Zustände, Zeit oder Aktionen. Schach ist diskret (feste Felder), Taxi fahren ist kontinuierlich.
         \end{itemize}
\end{enumerate}

\subsection{Agententypen}

Es gibt vier grundlegende Typen von Agentenprogrammen, aufsteigend nach Komplexität und Fähigkeit.

\subsubsection{1. Simple Reflex Agents (Einfache Reflex-Agenten)}
Diese Agenten entscheiden nur basierend auf der \textit{aktuellen} Wahrnehmung. Sie ignorieren die Wahrnehmungshistorie.
\begin{itemize}
   \item \textbf{Funktionsweise:} Condition-Action Rules (Wenn-Dann-Regeln).
   \item \textbf{Formel:} Aktion = Funktion(Aktuelle Wahrnehmung).
   \item \textbf{Einschränkung:} Funktionieren nur zuverlässig in \textit{fully observable} Umgebungen. Drohen in unendliche Schleifen zu geraten, wenn die Umgebung nur teilweise beobachtbar ist.
\end{itemize}
\begin{center}
   \vcentered{\includegraphics[width=0.7\textwidth]{eiki_1_AI101-02_page_20_1.png}}
\end{center}

\subsubsection{2. Model-based Reflex Agents (Modellbasierte Reflex-Agenten)}
Diese Agenten besitzen einen internen Zustand (\textit{Internal State}), um mit partieller Beobachtbarkeit umzugehen.
\begin{itemize}
   \item \textbf{Interner Zustand:} Repräsentiert Wissen über die Welt, das aktuell nicht sichtbar ist (Gedächtnis).
   \item \textbf{Modell der Welt:} Wissen darüber, wie die Welt funktioniert (Übergangsmodelle) und wie Aktionen die Welt verändern.
   \item \textbf{Ablauf:} Update State $\rightarrow$ Wähle Regel $\rightarrow$ Aktion.
\end{itemize}
\begin{center}
   \vcentered{\includegraphics[width=0.7\textwidth]{eiki_1_AI101-02_page_22_1.png}}
\end{center}

\subsubsection{3. Goal-based Agents (Zielbasierte Agenten)}
Wissen über den Zustand reicht nicht aus; der Agent benötigt ein Ziel (\textit{Goal}).
\begin{itemize}
   \item \textbf{Planung/Suche:} Der Agent simuliert verschiedene Sequenzen von Aktionen, um zu sehen, ob sie zum Ziel führen.
   \item \textbf{Unterschied zum Reflex:} Der Reflex-Agent reagiert, der Goal-based Agent plant in die Zukunft.
   \item \textbf{Flexibilität:} Ziele können sich ändern, der Agent passt sein Verhalten an.
\end{itemize}
\begin{center}
   \vcentered{\includegraphics[width=0.7\textwidth]{eiki_1_AI101-02_page_24_1.png}}
\end{center}

\subsubsection{4. Utility-based Agents (Nutzenbasierte Agenten)}
Ziele sind oft binär (erreicht/nicht erreicht). Nutzen (\textit{Utility}) erlaubt eine feinere Unterscheidung.
\begin{itemize}
   \item \textbf{Utility Function:} Bildet einen Zustand auf eine reelle Zahl ab, die den Grad der ``Zufriedenheit'' des Agenten angibt.
   \item \textbf{Vorteil:} Ermöglicht rationale Entscheidungen bei
         \begin{itemize}
            \item Zielkonflikten (z.B. Geschwindigkeit vs. Sicherheit).
            \item Unsicherheit (Abwägung von Erfolgswahrscheinlichkeit und Nutzen).
         \end{itemize}
\end{itemize}
\begin{center}
   \vcentered{\includegraphics[width=0.7\textwidth]{eiki_1_AI101-02_page_26_1.png}}
\end{center}

\subsubsection{5. Learning Agents (Lernende Agenten)}
Jeder der obigen Agententypen kann lernfähig gemacht werden. Dies erlaubt dem Agenten, in unbekannten Umgebungen zu operieren und kompetenter zu werden.

\begin{defbox}[Komponenten eines lernenden Agenten]
   \begin{itemize}
      \item \textbf{Performance Element:} Der eigentliche Agent (einer der 4 obigen Typen), der Entscheidungen trifft.
      \item \textbf{Critic (Kritiker):} Bewertet, wie gut der Agent ist, basierend auf einem festen Leistungsstandard.
      \item \textbf{Learning Element:} Nutzt das Feedback des Kritikers, um das Performance Element zu verbessern.
      \item \textbf{Problem Generator:} Schlägt Aktionen vor, die zu neuen Erfahrungen führen (Exploration), auch wenn diese kurzfristig suboptimal sind.
   \end{itemize}
\end{defbox}
\begin{center}
   \vcentered{\includegraphics[width=0.7\textwidth]{eiki_1_AI101-02_page_28_1.png}}
\end{center}

\end{document}