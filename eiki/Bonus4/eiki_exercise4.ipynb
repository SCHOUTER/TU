{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e722a24",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "In this exercise you will implement a forward pass and backpropagation **by hand**, then do the same with **PyTorch**. <br>\n",
    "You will build a tiny neural network for digit classification and see how the loss changes after an update step. <br>\n",
    "**Requirements:** You need `torch` (for MNIST + PyTorch) and `scikit-learn` (fallback dataset). If MNIST download fails, the notebook will use sklearn's digits dataset instead. <br>\n",
    "First enter your name and matrikelnummer. Without those we can't give you points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1e7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Enter your matriculation number and name\n",
    "matrikelnummer = 3602227\n",
    "name = \"Niclas Kusenbach\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bbeb5a",
   "metadata": {},
   "source": [
    "The required imports for this exercise are as follows:\n",
    "numpy\n",
    "nbformat\n",
    "nbconvert\n",
    "torch\n",
    "torchvision\n",
    "scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faeb4ae",
   "metadata": {},
   "source": [
    "# MNIST Classification (NumPy + PyTorch)\n",
    "You will start from the **MNIST** dataset and implement a tiny neural network for classification.\n",
    "\n",
    "- Use the provided dataset loader and initial weights.\n",
    "- Keep variable names unchanged so the results are comparable.\n",
    "- **Important:** store intermediate results with the variable names listed in each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2baab",
   "metadata": {},
   "source": [
    "## Helper functions (provided)\n",
    "These helpers are available for the tasks. Feel free to use or ignore them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8471d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(x)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    oh = np.zeros((len(y), num_classes), dtype=np.float32)\n",
    "    oh[np.arange(len(y)), y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return (y_pred == y_true).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee5509",
   "metadata": {},
   "source": [
    "## Dataset & initialization (provided)\n",
    "We try to load MNIST. If MNIST is not available, we fall back to the sklearn digits dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d5525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_or_digits():\n",
    "    # Try torchvision MNIST\n",
    "    try:\n",
    "        import torch  # pytorch>=2.5.1\n",
    "        from torchvision import datasets, transforms\n",
    "        transform = transforms.ToTensor()\n",
    "        train_ds = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "        test_ds = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
    "\n",
    "        X_train = train_ds.data.numpy().astype(np.float32) / 255.0\n",
    "        y_train = train_ds.targets.numpy().astype(np.int64)\n",
    "        X_test = test_ds.data.numpy().astype(np.float32) / 255.0\n",
    "        y_test = test_ds.targets.numpy().astype(np.int64)\n",
    "\n",
    "        X_train = X_train.reshape(len(X_train), -1)\n",
    "        X_test = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "        # Use small subsets for speed\n",
    "        X_train = X_train[:2000]\n",
    "        y_train = y_train[:2000]\n",
    "        X_test = X_test[:500]\n",
    "        y_test = y_test[:500]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: sklearn digits (8x8)\n",
    "    try:\n",
    "        from sklearn.datasets import load_digits\n",
    "        digits = load_digits()\n",
    "        X = digits.data.astype(np.float32) / 16.0\n",
    "        y = digits.target.astype(np.int64)\n",
    "        # simple split\n",
    "        n = len(X)\n",
    "        split = int(0.8 * n)\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Could not load MNIST or digits dataset\") from e\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist_or_digits()\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# Tiny 2-layer network parameters (seeded for reproducibility)\n",
    "rng = np.random.default_rng(1)\n",
    "W1 = rng.normal(scale=0.1, size=(input_dim, 64))\n",
    "b1 = np.zeros(64)\n",
    "W2 = rng.normal(scale=0.1, size=(64, num_classes))\n",
    "b2 = np.zeros(num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186e867",
   "metadata": {},
   "source": [
    "## Task 0: Dataset preview\n",
    "Print the shape of `X_train` and the first 3 labels. This helps you verify the input size.\n",
    "Look at a few samples to understand the inputs and labels.\n",
    "Then lets take the first sample from our training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c33290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# TODO: print the shape of X_train and the first 3 labels\n",
    "print(X_train.shape)\n",
    "print(y_train[:3])\n",
    "\n",
    "# TODO: implement get_sample function\n",
    "def get_sample(X_train,Y_train,index):\n",
    "    x = X_train[index:index+1]\n",
    "    y = Y_train[index]\n",
    "    return x, y\n",
    "\n",
    "#Has to be wrapped in a method for grading purposes\n",
    "xb, yb_batch = get_sample(X_train, y_train, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bf962",
   "metadata": {},
   "source": [
    "## Task 1: One forward pass (NumPy)\n",
    "Pick a single image `x0` and compute:\n",
    "\n",
    "- hidden layer with ReLU (`z1`, `a1`)\n",
    "- logits (`logits`)\n",
    "- softmax probabilities (`probs`)\n",
    "- cross-entropy loss for the true label (`loss`)\n",
    "\n",
    "**Required variable names:** `x0`, `y0`, `z1`, `a1`, `logits`, `probs`, `loss`, `pred`.\n",
    "Print the predicted class and the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6cbcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 9, loss: 2.2566568225917716\n"
     ]
    }
   ],
   "source": [
    "# TODO: forward pass for a single example\n",
    "def forward_pass():\n",
    "    x0 = X_train[0:1]\n",
    "    y0 = y_train[0]\n",
    "    z1 = np.dot(x0, W1) + b1  #pre-activation\n",
    "    a1 = relu(z1) #activation function\n",
    "    logits = np.dot(a1, W2) + b2\n",
    "    probs = softmax(logits)\n",
    "    loss = -1 * np.log(probs[0, y0])\n",
    "    pred = np.argmax(probs)\n",
    "    return z1, a1, logits, probs, loss, pred\n",
    "    \n",
    "\n",
    "z1, a1, logits, probs, loss, pred = forward_pass()\n",
    "print(f\"pred: {pred}, loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a679e",
   "metadata": {},
   "source": [
    "## Task 2: One update step (NumPy)\n",
    "Compute gradients for the **single example** and do one SGD update.\n",
    "\n",
    "Hints:\n",
    "- `dlogits = probs; dlogits[0, y0] -= 1`\n",
    "- Backprop through ReLU and the two linear layers.\n",
    "\n",
    "**Required variable names:** `dlogits`, `dW2`, `db2`, `da1`, `dz1`, `dW1`, `db1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefa792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one update step\n",
    "def update_step(probs, y0, a1, z1, x0, W1, b1, W2, b2, lr=0.1):\n",
    "    dlogits = probs.copy()\n",
    "    dlogits[0, y0] -= 1\n",
    "    dW2 = np.dot(a1.T, dlogits)\n",
    "    db2 = dlogits.squeeze()\n",
    "    da1 = dlogits@W2.T\n",
    "    dz1 = da1.copy()\n",
    "    dz1[z1 <= 0] = 0\n",
    "    dW1 = np.dot(x0.T, dz1)\n",
    "    db1 = dz1.squeeze()\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    return W1, b1, W2, b2\n",
    "    \n",
    "\n",
    "W1, b1, W2, b2 = update_step(probs, yb_batch, a1, z1, xb, W1, b1, W2, b2, lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90991bf8",
   "metadata": {},
   "source": [
    "## Task 3: PyTorch autograd\n",
    "Re-implement the same single-example forward/backward pass in [PyTorch](https://pytorch.org/get-started/locally/). The goal is to reproduce the same forward pass as in Task 1/2, and then let PyTorch compute gradients. If you want you can compare these PyTorch gradients with your NumPy gradients to see they match (up to small numerical differences).\n",
    "\n",
    "\n",
    "**Required variable names:** `W1_t`, `b1_t`, `W2_t`, `b2_t`, `loss_t`.\n",
    "Call `loss_t.backward()` so the gradients are stored in `.grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64]) torch.Size([64]) torch.Size([64, 10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pytorch_forward_backward(x0, y0, W1, b1, W2, b2):\n",
    "    import torch\n",
    "    x0_t = torch.from_numpy(x0).float()\n",
    "    y0_t = torch.tensor([y0], requires_grad=False)\n",
    "    W1_t = torch.tensor(W1, requires_grad=True, dtype=torch.float32)\n",
    "    b1_t = torch.tensor(b1, requires_grad=True, dtype=torch.float32)\n",
    "    W2_t = torch.tensor(W2, requires_grad=True, dtype=torch.float32)\n",
    "    b2_t = torch.tensor(b2, requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    z1_t = torch.matmul(x0_t, W1_t) + b1_t\n",
    "    a1_t = torch.relu(z1_t)\n",
    "    logits_t = torch.matmul(a1_t, W2_t) + b2_t\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits_t, y0_t)\n",
    "    loss_t.backward()\n",
    "    \n",
    "    print(W1_t.grad.shape, b1_t.grad.shape, W2_t.grad.shape, b2_t.grad.shape)\n",
    "    \n",
    "    return W1_t, b1_t, W2_t, b2_t, loss_t\n",
    "   \n",
    "\n",
    "W1_t, b1_t, W2_t, b2_t, loss_t = pytorch_forward_backward(xb, yb_batch, W1, b1, W2, b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23e4bc",
   "metadata": {},
   "source": [
    "## Task 4: Mini training loop (NumPy)\n",
    "Train for a few epochs on a small subset and report accuracy on the test set. You can experiment with lr, epochs, and batch_size to see how they affect convergence.\n",
    "\n",
    "!!!Warning!!!\n",
    "The public test only works with the default values, and the given seed.\n",
    "\n",
    "\n",
    "At the end, store:\n",
    "- `loss_last` (the most recent batch loss)\n",
    "- `test_acc_last` (the most recent test accuracy computed on the full test set)\n",
    "\n",
    "Hint for test accuracy:\n",
    "- After each epoch, run a forward pass on `X_test` using your current `W1`, `b1`, `W2`, `b2`.\n",
    "- Compute logits for all test examples, turn them into predicted classes with `np.argmax(logits_t, axis=1)`\n",
    "  and then call `accuracy(preds, y_test)` to get a single scalar test accuracy.\n",
    "- Store this value in `test_acc_last` so it reflects the accuracy of the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3168b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.6168, Final test accuracy: 0.803\n"
     ]
    }
   ],
   "source": [
    "# TODO: training loop\n",
    "def training_loop(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W1,\n",
    "    b1,\n",
    "    W2,\n",
    "    b2,\n",
    "    lr=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    seed=42,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    loss_last = None\n",
    "    test_acc_last = None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        perm = np.random.permutation(len(X_train))  # shuffle\n",
    "        Xb = X_train[perm]\n",
    "        yb = y_train[perm]\n",
    "        for i in range(0, len(Xb), batch_size):\n",
    "            # TODO: inside this minibatch loop\n",
    "            xb = Xb[i : i + batch_size]\n",
    "            yb_batch = yb[i : i + batch_size]\n",
    "            # 1) forward pass: z1, a1, logits, probs for xb\n",
    "            z1 = np.dot(xb, W1) + b1  # pre-activation\n",
    "            a1 = relu(z1)  # activation function\n",
    "            logits = np.dot(a1, W2) + b2\n",
    "            probs = softmax(logits)\n",
    "\n",
    "            # 2) calculate cross-entropy loss over yb\n",
    "            loss = -1 * np.mean(np.log(probs[np.arange(len(xb)), yb_batch]))\n",
    "\n",
    "            # 3) backprop: dlogits, dW2, db2, da1, dz1, dW1, db1 (similar to Task 2 but batched)\n",
    "            dlogits = probs.copy()\n",
    "            dlogits[np.arange(len(xb)), yb_batch] -= 1\n",
    "\n",
    "            dW2 = np.dot(a1.T, dlogits) / len(xb)\n",
    "            db2 = np.sum(dlogits, axis=0) / len(xb)\n",
    "            da1 = dlogits @ W2.T\n",
    "            dz1 = da1.copy()\n",
    "            dz1[z1 <= 0] = 0\n",
    "            dW1 = np.dot(xb.T, dz1) / len(xb)\n",
    "            db1 = np.sum(dz1, axis=0) / len(xb)\n",
    "\n",
    "            pred = np.argmax(probs)\n",
    "            # 4) SGD update of W1, b1, W2, b2\n",
    "            W1 -= lr * dW1\n",
    "            b1 -= lr * db1\n",
    "            W2 -= lr * dW2\n",
    "            b2 -= lr * db2\n",
    "            # 5) set loss_last = loss for this minibatch\n",
    "            loss_last = loss\n",
    "        # After finishing all minibatches for this epoch, compute test accuracy.\n",
    "        # Hint: reuse the forward pass on the full test set:\n",
    "        #   [...]\n",
    "\n",
    "        z1_test = np.dot(X_test, W1) + b1\n",
    "        a1_test = relu(z1_test)\n",
    "        logits_test = np.dot(a1_test, W2) + b2\n",
    "\n",
    "        preds = np.argmax(logits_test, axis=1)\n",
    "        test_acc_last = accuracy(preds, y_test)\n",
    "    # Also keep loss_last as the loss from the last minibatch of this epoch.\n",
    "\n",
    "    return W1, b1, W2, b2, loss_last, test_acc_last\n",
    "\n",
    "\n",
    "W1, b1, W2, b2, loss_last, test_acc_last = training_loop(\n",
    "    X_train, y_train, X_test, y_test, W1, b1, W2, b2\n",
    ")\n",
    "\n",
    "print(f\"Final loss: {loss_last:.4f}, Final test accuracy: {test_acc_last:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128759d1",
   "metadata": {},
   "source": [
    "Note: <br>\n",
    "\n",
    "If you have an implementation that you believe is correct but that does not pass the public test, please contact us via the forum. There is some numerical leeway built into the tests, but we have not tested all possible implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
