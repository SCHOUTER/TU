\documentclass[
../../eicb_summary.tex,
]
{subfiles}

\externaldocument[ext:]{../../eicb_summary}

\begin{document}

\section{Syntaktische Analyse}

Diese Sektion behandelt den Prozess der Übersetzung von Quellcode in Maschinencode, mit besonderem Fokus auf die Phasen der syntaktischen Analyse (Scanner und Parser), Grammatiktransformationen und die Implementierung von Parsern mittels rekursivem Abstieg.

\subsection{Einordnung und Struktur von Compilern}

Die Übersetzung erfolgt in mehreren Phasen, die logisch voneinander getrennt sind. Physikalisch können diese in einem oder mehreren Durchgängen (Passes) organisiert sein.

\begin{defbox}[Terminologie]
  \begin{itemize}
    \item \textbf{Phase:} Ein logischer Schritt im Übersetzungsprozess (z.B. Syntaxanalyse, Codegenerierung).
    \item \textbf{Pass (Durchgang):} Ein kompletter Durchlauf über den Quelltext oder eine Zwischenrepräsentation (Intermediate Representation, IR). Ein Pass kann mehrere Phasen beinhalten.
  \end{itemize}
\end{defbox}

\subsubsection{Vergleich: Ein-Pass vs. Multi-Pass Compiler}

\begin{table}[h]
  \centering
  \begin{tabular}{|l|p{6cm}|p{6cm}|}
    \hline
    \textbf{Kriterium}    & \textbf{Ein-Pass Compiler}                                                               & \textbf{Multi-Pass Compiler}                                                             \\ \hline
    \textbf{Arbeitsweise} & Führt Syntaxanalyse, Kontextanalyse und Codegenerierung verschränkt aus. Keine echte IR. & Mehrere Durchläufe. Datenübergabe zwischen Passes erfolgt über IR (z.B. AST).            \\ \hline
    \textbf{Laufzeit}     & \textbf{+} Schnell (weniger I/O-Overhead).                                               & \textbf{-} Langsamer durch mehrfaches Lesen/Schreiben der IR.                            \\ \hline
    \textbf{Speicher}     & \textbf{+} Geringer Speicherbedarf (gut für große Programme bei wenig RAM).              & \textbf{+} Besser für kleine Programme; bei großen Programmen speicherintensiv durch IR. \\ \hline
    \textbf{Modularität}  & \textbf{--} Schlecht trennbar, "Spaghetti-Code".                                         & \textbf{+} Klare Trennung der Belange.                                                   \\ \hline
    \textbf{Flexibilität} & \textbf{-} Starr.                                                                        & \textbf{+} Austauschbare Backends/Frontends möglich.                                     \\ \hline
    \textbf{Optimierung}  & \textbf{--} Nur lokale Optimierungen möglich.                                            & \textbf{+} Globale Optimierungen auf IR möglich.                                         \\ \hline
    \textbf{Kontext}      & Definitionen müssen oft \emph{vor} der Verwendung stehen (z.B. Pascal).                  & Auflösung von Vorwärtsreferenzen (z.B. Java) problemlos möglich.                         \\ \hline
  \end{tabular}
\end{table}

\subsection{Ablauf der Syntaxanalyse}

Die Syntaxanalyse ist der Kern des Frontends und transformiert den linearen Zeichenstrom in eine strukturierte Repräsentation.

% Grafik: Ablauf Source -> Scanner -> Token Stream -> Parser -> AST
% (Entspricht Folie 12)

\begin{enumerate}
  \item \textbf{Scanner (Lexikalische Analyse):}
        \begin{itemize}
          \item Eingabe: Zeichenfolge (Source Code).
          \item Aufgabe: Gruppierung von Zeichen zu \defc{Tokens} (atomare Symbole wie Schlüsselwörter, Bezeichner, Literale).
          \item Filterung: Entfernt Whitespace und Kommentare.
        \end{itemize}

  \item \textbf{Parser (Syntaktische Analyse):}
        \begin{itemize}
          \item Eingabe: Token-Stream.
          \item Aufgabe: Überprüfung der grammatikalischen Struktur gemäß einer kontextfreien Grammatik (CFG).
          \item Ausgabe: Abstract Syntax Tree (AST) oder Fehlermeldungen.
        \end{itemize}
\end{enumerate}

\begin{defbox}[Token]
  Ein Token ist ein Tupel bestehend aus:
  \begin{itemize}
    \item \textbf{Kind:} Die Art des Tokens (z.B. \texttt{IDENTIFIER}, \texttt{INTLITERAL}, \texttt{IF}, \texttt{PLUS}).
    \item \textbf{Spelling:} Der tatsächliche Textinhalt (z.B. "x", "42", "if", "+").
    \item \textbf{Position:} Zeile und Spalte im Quelltext (für Fehlermeldungen).
  \end{itemize}
\end{defbox}

\subsection{Grammatiken und Transformationen}

Die Syntax von Programmiersprachen wird meist durch kontextfreie Grammatiken (CFG) spezifiziert.

\subsubsection{Notation}
\begin{itemize}
  \item \textbf{BNF (Backus-Naur-Form):} Klassische Notation.
  \item \textbf{EBNF (Extended BNF):} Erlaubt reguläre Ausdrücke auf der rechten Seite für kompaktere Schreibweise.
        \begin{itemize}
          \item \texttt{(\ldots)}: Gruppierung
          \item \texttt{|}: Alternative
          \item \texttt{[\ldots]} oder \texttt{?}: Optional (0 oder 1 Mal)
          \item \texttt{\char123\ldots\char125} oder \texttt{*}: Wiederholung (0 bis n Mal)
        \end{itemize}
\end{itemize}

\subsubsection{Notwendige Grammatik-Transformationen für Parser}
Für bestimmte Parsing-Techniken (insbesondere Top-Down / Recursive Descent) muss die Grammatik angepasst werden, ohne die definierte Sprache zu ändern.

\paragraph{1. Linksausklammern (Left Factoring)}
Problem: Der Parser kann sich nicht entscheiden, welche Produktion er wählen soll, da mehrere mit demselben Symbol beginnen.
\\
\textbf{Regel:} $X ::= \alpha \beta \mid \alpha \gamma \quad \Rightarrow \quad X ::= \alpha (\beta \mid \gamma)$
\\
\textit{Beispiel:}
\begin{align*}
  Cmd & ::= \texttt{if} \; E \; \texttt{then} \; C                        \\
      & \mid \texttt{if} \; E \; \texttt{then} \; C \; \texttt{else} \; C
\end{align*}
wird zu:
\[Cmd ::= \texttt{if} \; E \; \texttt{then} \; C \; (\epsilon \mid \texttt{else} \; C)\]

\paragraph{2. Beseitigung von Linksrekursion}
Problem: Ein Top-Down-Parser gerät in eine Endlosschleife, wenn ein Nicht-Terminal sich selbst als erstes Symbol aufruft ($N \Rightarrow N\alpha$).

\textbf{Direkte Linksrekursion:} $N ::= N \alpha \mid \beta$

\textbf{Lösung (Transformation in Rechtsrekursion oder Iteration):}
$$ N ::= \beta (\alpha)^* $$
bzw. formal in BNF:
\begin{align*}
  N  & ::= \beta N'                \\
  N' & ::= \alpha N' \mid \epsilon
\end{align*}

\subsection{Parsing-Strategien}

Man unterscheidet zwei grundlegende Herangehensweisen, um den Ableitungsbaum (Parse Tree) zu konstruieren.

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{|l|p{4.5cm}|p{4.5cm}|}
    \hline
                       & \textbf{Top-Down (z.B. LL)}                                & \textbf{Bottom-Up (z.B. LR)}                                                    \\ \hline
    \textbf{Richtung}  & Wurzel $\to$ Blätter                                       & Blätter $\to$ Wurzel                                                            \\ \hline
    \textbf{Vorgehen}  & Expandiert das am weitesten links stehende Nicht-Terminal. & "Shift" (Einlesen) und "Reduce" (Ersetzen der rechten Seite durch linke Seite). \\ \hline
    \textbf{Lookahead} & Wählt Produktion anhand der nächsten $k$ Token.            & Entscheidungen basieren auf Stack-Zustand und Lookahead.                        \\ \hline
    \textbf{Eignung}   & Intuitive Implementierung von Hand (Rekursiver Abstieg).   & Mächtiger, handhabt Linksrekursion, meist generiert (Yacc, Bison).              \\ \hline
  \end{tabular}
\end{table}

\begin{defbox}[LL(k)]
  Eine Grammatik ist $LL(k)$, wenn der Parser beim Lesen von links nach rechts (\textbf{L}) und beim Konstruieren einer Linksanleitung (\textbf{L}) mit $k$ Symbolen Vorausschau (Lookahead) immer eindeutig die nächste Produktion bestimmen kann.
\end{defbox}

\subsection{Rekursiver Abstieg (Recursive Descent)}

Dies ist eine direkte Implementierung eines Top-Down-Parsers.
\begin{itemize}
  \item Jedes Nicht-Terminal der Grammatik wird zu einer Methode (z.B. \texttt{parseStatement()}).
  \item Die rechte Seite der Produktion bestimmt den Rumpf der Methode.
  \item Terminale werden mit dem aktuellen Token verglichen (\texttt{accept}).
  \item Nicht-Terminale führen zu rekursiven Aufrufen der entsprechenden Methoden.
\end{itemize}

\textbf{Struktur einer Parse-Methode:}
\begin{verbatim}
void parseN() {
  if (currentToken in Starters[Alternative1]) {
    parseAlternative1();
  } else if (currentToken in Starters[Alternative2]) {
    parseAlternative2();
  } else {
    error("Syntax Error");
  }
}
\end{verbatim}

\subsection{LL(1)-Analyse und Entscheidungsmengen}

Damit ein rekursiver Abstieg deterministisch funktioniert (ohne Backtracking), muss die Grammatik die LL(1)-Eigenschaft erfüllen. Dazu werden Hilfsmengen berechnet.

\subsubsection{Starters (First) Menge}
$Starters[[X]]$ ist die Menge aller Terminalsymbole, mit denen ein aus $X$ ableitbarer Satz beginnen kann.
\begin{itemize}
  \item $Starters[[t]] = \{t\}$ (für Terminal $t$)
  \item $Starters[[\epsilon]] = \emptyset$ (Achtung: $\epsilon$ wird oft separat behandelt)
  \item $Starters[[XY]] = Starters[[X]]$, falls $X$ nicht zu $\epsilon$ werden kann. Sonst $Starters[[X]] \cup Starters[[Y]]$.
  \item $Starters[[X|Y]] = Starters[[X]] \cup Starters[[Y]]$
\end{itemize}

\subsubsection{Follow Menge}
$Follow[[N]]$ ist die Menge aller Terminalsymbole, die in irgendeiner Satzform direkt \emph{nach} dem Nicht-Terminal $N$ stehen können.
\begin{itemize}
  \item Wird benötigt, wenn eine Produktion zu $\epsilon$ (leer) abgeleitet werden kann.
  \item Regel: Wenn $A ::= \alpha B \beta$, dann ist alles in $Starters[[\beta]]$ auch in $Follow[[B]]$.
  \item Wenn $\beta \to \epsilon$ (oder $B$ am Ende steht: $A ::= \alpha B$), dann ist alles in $Follow[[A]]$ auch in $Follow[[B]]$.
\end{itemize}

\subsubsection{Director Sets (Entscheidungsmengen) und LL(1)-Bedingungen}
Um zwischen Alternativen $A ::= \alpha \mid \beta$ zu wählen, nutzen wir das \defc{Director Set} $Dir$.

$$ Dir[[\alpha]] = \begin{cases} Starters[[\alpha]] & \text{falls } \epsilon \notin Starters[[\alpha]] \\ Starters[[\alpha]] \cup Follow[[A]] & \text{falls } \epsilon \in Starters[[\alpha]] \end{cases} $$

\textbf{Bedingung für LL(1):}
Für alle Produktionen $A ::= \alpha_1 \mid \alpha_2 \mid \dots \mid \alpha_n$ muss gelten:
$$ Dir[[\alpha_i]] \cap Dir[[\alpha_j]] = \emptyset \quad \text{für alle } i \neq j $$
Das bedeutet:
\begin{itemize}
  \item Die Anfangssymbole der Alternativen müssen disjunkt sein.
  \item Falls eine Alternative leer sein kann ($\epsilon$), darf keines der Symbole, die \emph{nach} dem Nicht-Terminal folgen können, in den Anfangsmengen der anderen Alternativen enthalten sein.
\end{itemize}

\subsection{Abstract Syntax Tree (AST)}

Der AST ist eine komprimierte Version des Parse Trees. Er enthält keine unnötigen Details der konkreten Syntax (wie Klammern, Semikolons oder Hilfs-Nicht-Terminale aus der Grammatiktransformation), sondern repräsentiert die logische Struktur des Programms.

\textbf{Implementierung:}
\begin{itemize}
  \item Eine abstrakte Basisklasse \texttt{AST}.
  \item Unterklassen für Sprachkonstrukte (z.B. \texttt{Command}, \texttt{Expression}, \texttt{Declaration}).
  \item Konkrete Klassen für Varianten (z.B. \texttt{IfCmd}, \texttt{WhileCmd}, \texttt{BinaryExpr}).
  \item \textbf{Aufbau:} Die Parse-Methoden werden von \texttt{void} auf den Rückgabetyp \texttt{AST} geändert. Sie erzeugen Knoten und geben diese an den Aufrufer zurück ("bottom-up" Aufbau während der Rekursion).
\end{itemize}

% Grafik: AST Strukturbeispiel (Folie 68/73)
% Zeigt, wie z.B. eine Sequenz von Befehlen als geschachtelte Objekte (SeqCmd) repräsentiert wird.

\subsection{Lexikalische Analyse (Scanner-Implementierung)}

Der Scanner (Lexer) ist oft als endlicher Automat (Finite State Machine) realisiert oder wird manuell implementiert.

\subsubsection{Aufgaben}
\begin{enumerate}
  \item Lesen der Eingabezeichen.
  \item Überlesen von "Whitespace" (Leerzeichen, Tabs, Newlines) und Kommentaren.
  \item Erkennen des längstmöglichen passenden Tokens (Longest Match).
  \item Unterscheidung zwischen Bezeichnern (Identifiers) und Schlüsselwörtern (Keywords).
\end{enumerate}

\subsubsection{Behandlung von Schlüsselwörtern}
Da Schlüsselwörter (z.B. \texttt{if}, \texttt{while}) lexikalisch oft wie Bezeichner aussehen, werden sie zunächst als \texttt{IDENTIFIER} gescannt.
\\
\textbf{Best Practice:}
\begin{itemize}
  \item Scanne das Wort als Identifier.
  \item Prüfe in einer Hash-Map/Tabelle, ob der Text ein reserviertes Wort ist.
  \item Wenn ja: Ändere die Token-Art (z.B. von \texttt{IDENTIFIER} zu \texttt{IF}).
\end{itemize}

\subsubsection{Schnittstelle zum Parser}
\begin{itemize}
  \item \texttt{scan()}: Liefert das nächste Token.
  \item \texttt{currentKind}: Art des aktuellen Tokens.
  \item \texttt{currentSpelling}: Textinhalt des aktuellen Tokens.
\end{itemize}

\end{document}